{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkQFTgKJiqsy"
   },
   "source": [
    "# Урок 2. Масштабирование признаков. Регуляризация. Стохастический градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**План занятия**\n",
    "\n",
    "* [Теоретическая часть](#theory)\n",
    "    * [Масштабирование признаков](#0)\n",
    "    * [Стохастический градиентный спуск](#1)\n",
    "    * [SGD своими руками](#sgd_manual)\n",
    "    * [Переобучение](#overfit)\n",
    "    * [Методы борьбы с переобучением](#methods)\n",
    "        * [Регуляризация](#reg)\n",
    "* [Практическая часть](#practice)\n",
    "    * [Домашнее задание](#hw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Теоретическая часть<a class=\"anchor\" id=\"theory\"></a><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLVfsJXviqs1"
   },
   "source": [
    "## Масштабирование признаков <a class='anchor' id='0'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu5ocQDMiqs2"
   },
   "source": [
    "В машинном обучении при работе с линейными моделями полезной является практика _масштабирования признаков_. Многие методы машинного обучения, в том числе и линейные, наиболее эффективны в том случае, когда признаки имеют одинаковый масштаб. По сути масштабирование означает приведение признаков к какой-то единой шкале. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsw78QW7iqs3"
   },
   "source": [
    "Существует большое количество методов масштабирования, наиболее популярными из которых являются _нормализация_ и _стандартизация_.\n",
    "\n",
    "Метод **нормализации** заключается в приведении признаков к масштабу в диапазоне [0-1].\n",
    "\n",
    "Для его реализации необходимо найти минимальное $min_{j} (x^{j}_{i})$ и максимальное $max_{j} (x^{j}_{i})$ значение признака на обучающей выборке. При этом отмасштабированное значение признака будет находиться по формуле\n",
    "\n",
    "$$x^{j}_{i} = \\frac{x^{j}_{i} - min_{j} (x^{j}_{i})}{max_{j} (x^{j}_{i})-min_{j} (x^{j}_{i})}.$$\n",
    "\n",
    "После преобразования значений признаков минимальное значение превратится в 0, а максимальное - в 1.\n",
    "\n",
    "Пример различия в сходимости алгоритма на сырых и нормализованных данных:\n",
    "<img src=\"images/L2_normalization.png\" style=\"width: 500px;\">\n",
    "\n",
    "**Стандартизация** заключается в получении своего рода значения сдвига каждого признака от среднего. Для ее реализации необходимо вычислить среднее значение признака \n",
    "\n",
    "$$\\mu_{j} = \\frac{1}{l}\\sum^{l}_{i=1}x^{j}_{i}$$\n",
    "\n",
    "и стандартное отклонение, которое находится путем суммирования квадратов отклонения значений признака на объектах выборки от среднего $\\mu_{j}$ и делением на число объектов выборки с последующим извлечением корня:\n",
    "\n",
    "$$\\sigma_{j} = \\sqrt{\\frac{1}{l}\\sum^{l}_{i=1}(x^{j}_{i}-\\mu_{j})^{2}}$$\n",
    "\n",
    "Чтобы отмасштабировать признак, каждое его значение преобразуется по формуле\n",
    "\n",
    "$$x^{j}_{i}=\\frac{x^{j}_{i} - \\mu_{j}}{\\sigma_{j}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s32Rv-eFiqs4"
   },
   "source": [
    "Масштабирование является важным этапом подготовки данных перед применением методов машинного обучения. \n",
    "\n",
    "Важным и последним свойством масштабирования является факт, что после масштабирования признаков в линейных моделях веса при них могут интерпретироваться как мера значимости этих признаков.\n",
    "\n",
    "Существуют различные ситуации, когда целесообразно применять тот или иной метод масштабирования. Нормализовать полезно признаки, опирающиеся на величину значений - такие как расстояние (knn, k-means). Стандартизировать полезно признаки для модели, которая опирается на распределение (линейные модели). В общем случае, когда выбор метода неочевиден, полезной практикой считается создавать масштабированные копии набора данных, с которыми работает специалист, и сравнивать друг с другом полученные после применения модели результаты для выявления оптимального метода масштабирования для имеющейся ситуации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMcddg8niqs5"
   },
   "source": [
    "## Стохастический градиентный спуск <a class='anchor' id='1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X)$\n",
    "\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkzFJVniiqs6"
   },
   "source": [
    "Вспомним метод градиентного спуска, рассмотренный ранее.\n",
    "\n",
    "На каждой итерации приближение получается вычитанием из предыдущего вектора градиента, умноженного на некоторый шаг:\n",
    "\n",
    "\n",
    "$$w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X).$$\n",
    "\n",
    "При этом выражение градиента в матричной форме выглядит так:\n",
    "\n",
    "$$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$$\n",
    "\n",
    "Если расписать $j$-ю компонетну этого градиента, то получим\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w_{j}} = \\frac{2}{l}\\sum^{l}_{i=1}x^{j}_{i}(\\left \\langle w,x_{i} \\right \\rangle - y_{i}),$$\n",
    "\n",
    "то есть суммирование по всем $l$ объектам обучающей выборки. Здесь выражение под суммой показывает, как нужно изменить $j$-й вес, чтобы как можно сильнее улучшить качество __на объекте $x_{i}$__, а вся сумма показывает, как нужно изменить вес, чтобы улучшить качество на __всей выборке__.\n",
    "\n",
    "В этой формуле отражен один из главных недостатков градиентного спуска: если выборка большая по объему, то даже один шаг градиентного спуска будет занимать много вычислительных ресурсов и времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xlcthp9Uiqs7"
   },
   "source": [
    "Стремление к оптимизации процесса привело к появлению _стохастического градиентного спуска_ (Stochastic gradient descent, SGD). Идея его основана на том, что на одной итерации мы вычитаем не вектор градиента, вычисленный по всей выборке, а вместо этого случайно выбираем один объект из обучающей выборки $x_{i}$ и вычисляем градиент только на этом объекте, то есть градиент только одного слагаемого в функционале ошибки и вычитаем именно этот градиент из текущего приближения вектора весов:\n",
    "\n",
    "$$w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, \\{x_{i}\\}),$$\n",
    "\n",
    "то есть $\\nabla Q(w^{k-1}, X)$ заменяется на $\\nabla Q(w^{k-1}, \\{x_{i}\\})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mKZbftTiqs9"
   },
   "source": [
    "Если в случае градиентного спуска мы стараемся на каждой итерации уменьшить ошибку на всей выборке, и по мере увеличения числа итераций ошибка падает монотонно, то в случае стохастического градиентного спуска мы уменьшаем на каждой итерации ошибку только на одном объекте, но при этом есть вероятность увеличить ее на другом объекте, поэтому график изменения ошибки может получаться немонотонным, и даже иметь пики (см. пример по ссылке [1] из списка литературы). То есть на какой-то итерации мы можем даже увеличить ошибку, но при этом в целом по ходу метода ошибка снижается, и рано или поздно мы выходим на нормальный уровень."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * Выбираем случайные объект $x_{i}$ из X\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, \\{x_{i}\\})$\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/compare_gd_sgd.png\" width=550px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD своими руками<a class=\"anchor\" id=\"sgd_manual\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BBJDLSQiqs-"
   },
   "source": [
    "Реализуем стохастический градиентный спуск своими руками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "id": "c7NGnJrtiqs_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L92vtZuDiqtD"
   },
   "outputs": [],
   "source": [
    "# сгенерируем набор данных\n",
    "X, Y, coef = datasets.make_regression(n_samples=1000, n_features=2, n_informative=2, n_targets=1, \n",
    "                                      noise=5, coef=True, random_state=2)\n",
    "X[:, 0] *= 10\n",
    "display(X, Y, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], Y)\n",
    "\n",
    "ax.set_xlabel('X0')\n",
    "ax.set_ylabel('X1')\n",
    "ax.set_zlabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mq1B_FniqtF"
   },
   "source": [
    "Отмасштабируем получившиеся признаки методом стандартизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOKEKtFKiqtG"
   },
   "outputs": [],
   "source": [
    "# Получим средние значения и стандартное отклонение по столбцам\n",
    "\n",
    "means = np.mean(X, axis=0)\n",
    "stds = np.std(X, axis=0)\n",
    "# параметр axis указывается для вычисления значений по столбцам, а не по всему массиву\n",
    "display(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычтем каждое значение признака из среднего и поделим на стандартное отклонение\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        X[i][j] = (X[i][j] - means[j]) / stds[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X, axis=0)\n",
    "stds = np.std(X, axis=0)\n",
    "\n",
    "display(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BMCZ7uNiqtJ"
   },
   "outputs": [],
   "source": [
    "# реализуем функцию, определяющую среднеквадратичную ошибку\n",
    "def mserror(X, w, y_pred):\n",
    "    y = X.dot(w)\n",
    "    return (sum((y - y_pred)**2)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFPTJccEiqtO"
   },
   "source": [
    "Подготовка данных и средств проверки закончена. Далее реализуем сам стохастический градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwIeP5Y4iqtQ",
    "outputId": "d4ef12c1-fb3e-49c3-ebe3-31b35b53e902",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# инициализируем начальный вектор весов\n",
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "# список векторов весов после каждой итерации\n",
    "w_list = [w.copy()]\n",
    "\n",
    "# список значений ошибок после каждой итерации\n",
    "errors = []\n",
    "\n",
    "# шаг градиентного спуска\n",
    "eta = 0.01\n",
    "\n",
    "# максимальное число итераций\n",
    "max_iter = 1e3\n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "min_weight_dist = 1e-8\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_dist = np.inf\n",
    "\n",
    "# счетчик итераций\n",
    "iter_num = 0\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# ход градиентного спуска\n",
    "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
    "    \n",
    "    # генерируем случайный индекс объекта выборки\n",
    "    train_ind = np.random.randint(X.shape[0], size=1)\n",
    "    \n",
    "    y_pred = np.dot(X[train_ind], w)\n",
    "    new_w = w - eta * 2 / Y[train_ind].shape[0] * np.dot(X[train_ind].T, y_pred - Y[train_ind])\n",
    "\n",
    "    weight_dist = np.linalg.norm(new_w - w, ord=2)\n",
    " \n",
    "    error = mserror(X, new_w, Y)\n",
    "    \n",
    "    w_list.append(new_w.copy())\n",
    "    errors.append(error)\n",
    "    \n",
    "    if iter_num % 100 == 0:\n",
    "        print(f'Iteration #{iter_num}: W_new = {new_w}, MSE = {round(error, 2)}')\n",
    "\n",
    "    iter_num += 1\n",
    "    w = new_w\n",
    "    \n",
    "w_list = np.array(w_list)\n",
    "\n",
    "print(f'Iter {iter_num}: error - {error}, weights: {new_w}')\n",
    "print(f'В случае использования стохастического градиентного спуска ошибка составляет {round(errors[-1], 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-cN_umsiqtU"
   },
   "source": [
    "Для стохастического градиентного спуска мы увеличили максимальное число итераций (max_iter) до 1000, что естественно, так как из-за специфики метода для достижения сходимости нужно большее количество шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mq8gHc1iiqtU",
    "outputId": "25bd89c1-1ea5-47cd-8131-13371cad8077"
   },
   "outputs": [],
   "source": [
    "# Визуализируем изменение весов (красной точкой обозначены истинные веса, сгенерированные вначале)\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.title('Stochastic gradient descent')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.scatter(coef[0], coef[1], c='r')\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXCKz_wliqtZ",
    "outputId": "b57d60a0-b120-45b2-a4bc-5f70fbbdfa4f"
   },
   "outputs": [],
   "source": [
    "# Визуализируем изменение функционала ошибки\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.title('MSE')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QchXLomyiqte"
   },
   "source": [
    "Как и в случае градиентного спуска, вектор весов приближается к истинному. При этом падает и ошибка.\n",
    "\n",
    "Добиться лучшей скорости сходимости в методе стохастического градиентного спуска можно варьируя величину шага или используя методы, подбирающие ее адаптивно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_19JViDiqtg"
   },
   "source": [
    "Среди преимуществ SGD можно выделить гораздо более быстрое вычисление одного шага по сравнению с обычным градиентным спуском и отсутствие необходимости хранить всю выборку в памяти при работе метода, что в свою очередь позволяет работать с очень большими выборками, которые невозможно поместить в память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZXExx-Yiqth"
   },
   "source": [
    "## Переобучение <a class='anchor' id='overfit'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB93X534iqth"
   },
   "source": [
    "Чтобы понять смысл переобучения и недообучения, начнем с примера. Допустим, у нас есть исходная известная зависимость 3-го порядка:\n",
    "\n",
    "$$f(x) = 0.6 - 13.2x - 5.3 x^{2} - 4.17x^{3}.$$\n",
    "\n",
    "Реализуем ее в виде python-функции и построим график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x7O02fBiqti"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.6 - 13.2 * x - 5.3 * x ** 2 - 4.17 * x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZU05q4miqtk",
    "outputId": "2ff55331-ed9c-4a9c-d13e-15634e8f5643"
   },
   "outputs": [],
   "source": [
    "dots = np.linspace(-10, 10, 100)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-5000, 5000)\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.plot(dots, f(dots), color='g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY1HQExXiqtn"
   },
   "source": [
    "Теперь сгенерируем датасет из десяти случайных точек, подчиняющихся этой зависимости, с добавлением шума и нанесем на график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8aLmQt2iqto"
   },
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "x_data = np.random.uniform(-10, 10, 10)\n",
    "f_data = [f(i) for i in x_data] + np.random.uniform(-1000, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtY-zFVaiqtq",
    "outputId": "b7a8cc00-baea-44a8-9242-dab8943ecc41"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-5000, 5000)\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.plot(dots, f(dots), color='g')\n",
    "plt.scatter(x_data, f_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMHWU0yAiqt0"
   },
   "source": [
    "Теперь попробуем создать модель, способную восстановить исходную зависимость. Самым примитивным так называемый __константный алгоритм__, то есть модель вида \n",
    "\n",
    "$$a(x) = w_{0}.$$\n",
    "\n",
    "Зависимость такой модели от признаков будет иметь вид горизонтальной прямой, что, очевидно, не обобщает нашу зависимость в должном виде (см. график ниже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mYrEwYtiqt0",
    "outputId": "64e1758a-e946-4cc2-d346-d10da99d55ae"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-5000, 5000)\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.plot(dots, f(dots), color='g')\n",
    "plt.scatter(x_data, f_data)\n",
    "plt.plot(dots, [dots.mean()]*len(dots), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGkSELuTiqt3"
   },
   "source": [
    "Далее усложним семейство алгоритмов, применив линейную регрессию, которая в случае одного признака будет иметь вид \n",
    "\n",
    "$$a(x) = w_{0} + w_{1}x.$$\n",
    "\n",
    "Обучим соответствующую модель, применив для этого методы python \"из коробки\" для работы с линейной регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIhKhnC0iqt4",
    "outputId": "647aa2f0-ffd2-4598-c332-ffdb1c76cb8f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# создадим модель\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# обучим ее\n",
    "linear_regressor.fit(np.reshape(x_data, (-1, 1)), f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFLiACBZiqt7",
    "outputId": "03d85c51-0059-4ee3-902a-a037402d2ac5"
   },
   "outputs": [],
   "source": [
    "# выведем полученный вес при признаке и свободный коэффициент\n",
    "print(linear_regressor.coef_[0], linear_regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TEVNRsgiquB"
   },
   "source": [
    "Нанесем полученную после обучения модель на график"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmbR6vRniquD",
    "outputId": "db673d33-8110-402a-e2db-ef73e092b77f"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-5000, 5000)\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.plot(dots, f(dots), color='g')\n",
    "plt.scatter(x_data, f_data)\n",
    "plt.plot(dots, linear_regressor.predict(np.reshape(dots, (-1, 1))), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSKKmTmiquJ"
   },
   "source": [
    "Мы обучили линейную модель, и видимо, что она, как и константная, плохо восстанавливает исходную зависимость. В данном случае можно говорить о __недообучении__. Хороший алгоритм не был построен, поскольку с помощью выбранного семейства алгоритмов невозможно восстановить исходную закономерность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km9OjqjGiquK"
   },
   "source": [
    "Усложним используемое семейство алгоритмов до кубической зависимости \n",
    "\n",
    "$$a(x) = w_{0} + w_{1}x + w_{2}x^{2} + w_{3}x^{3}.$$\n",
    "\n",
    "Сделаем это путем искусственной генерации новой матрицы признаков, состоящей из исходных $x$, возведенных в степени до 3, используя `sklearn.preprocessing.PolynomialFeatures`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из [статьи](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) про PolynomialFeatures:\n",
    "\n",
    "For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[2, 3, 4]], columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree=2).fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree=3).fit_transform(x_data[0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[0]**0, x_data[0]**1, x_data[0]**2, x_data[0]**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDLG2XuViquL",
    "outputId": "2bd60151-46f9-4b64-f1f4-d21da948095a"
   },
   "outputs": [],
   "source": [
    "# создадим новую кубическую модель\n",
    "third_degree_regressor = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
    "\n",
    "# обучим ее\n",
    "third_degree_regressor.fit(np.reshape(x_data, (-1, 1)), f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kY0H8KPiquS",
    "outputId": "b6cdedec-6da3-4bb3-e8c8-077a3d0713ff"
   },
   "outputs": [],
   "source": [
    "# выведем полученные веса при признаках и свободный коэффициент\n",
    "print(third_degree_regressor.named_steps.linearregression.coef_)\n",
    "print(third_degree_regressor.named_steps.linearregression.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBKM6B6wiquV"
   },
   "source": [
    "Нанесем полученную в итоге зависимость на график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rKHJTjJiquW",
    "outputId": "7c012ee6-6aa2-426d-ce4d-15316b59f3c7"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-5000, 5000)\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.plot(dots, f(dots), color='g')\n",
    "plt.scatter(x_data, f_data)\n",
    "plt.plot(dots, third_degree_regressor.predict(np.reshape(dots, (-1, 1))), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VJdL3rhiquY"
   },
   "source": [
    "Полученный алгоритм достаточно хорошо описывает данные, но не идеально. И в реальных условиях может возникнуть вопрос, можно ли добиться лучшего совпадения увеличением сложности алгоритма.\n",
    "\n",
    "Проиллюстрируем, что происходит в случае использования многочлена 8-й степени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vc8pOjbOiquZ",
    "outputId": "f456a08c-224d-4bb9-d633-bcc6e0647dfb"
   },
   "outputs": [],
   "source": [
    "# создадим модель 8-й степени\n",
    "eighth_degree_regressor = make_pipeline(PolynomialFeatures(degree=8), LinearRegression())\n",
    "\n",
    "# обучим ее\n",
    "eighth_degree_regressor.fit(np.reshape(x_data, (-1, 1)), f_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8dlT6n5iquc"
   },
   "source": [
    "Покажем получившийся график зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выведем полученные веса при признаках и свободный коэффициент\n",
    "print(eighth_degree_regressor.named_steps.linearregression.coef_)\n",
    "print(eighth_degree_regressor.named_steps.linearregression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW7kvdoFiqud",
    "outputId": "96550f3e-f944-400b-fe2e-3e2890353751",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-5000, 5000)\n",
    "plt.xlim(-10, 10)\n",
    "\n",
    "plt.plot(dots, f(dots), color='g')\n",
    "plt.scatter(x_data, f_data)\n",
    "plt.plot(dots, eighth_degree_regressor.predict(np.reshape(dots, (-1, 1))), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы борьбы с переобучением <a class='anchor' id='methods'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkFoaeMCiquk"
   },
   "source": [
    "Видно, что новая модель лучше описывает имеющиеся в обучающей выборке данные и дает фактически идеальные ответы на них, но про этом в целом зависимость сильно отличается от истинной. Поэтому если мы попробуем применить эту модель на новых данных, ответы будут расходиться с правильными. Такое явление и называется __переобучением__. Алгоритм слишком сильно подогнан под обучающую выборку, и за счет этого будет давать неадекватные ответы на новых точках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CErR0qKOiquk"
   },
   "source": [
    "Таким образом, недообучение несет за собой плохое качество на обучении и на новых данных, а переобучение - хорошее качество на обучении и плохое на новых данных.\n",
    "\n",
    "Понятно, как бороться с недообучением - усложнять семейство алгоритмов. Возникает вопрос, как выявить переобучение и его избежать. В случае переобучения, как было сказано ранее, данные из обучающей выборки алгоритмом будут описываться хорошо, а новые данные - плохо, поэтому используя только обучающую выборку, невозможно заключить, хорошо обучен алгоритм или переобучен, так как оба они будут хорошо описывать известные данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9MMDQHDiqul"
   },
   "source": [
    "Есть несколько методов оценки качества алгоритма и выявления переобучения:\n",
    "\n",
    "1. Не использовать всю выборку для обучения, а откладывать часть данных для проверки полученного алгоритма. Это называется **отложенной выборкой**. Данные делятся на обучающую и тестовую выборку в соотношении, например, 0.7 к 0.3, и затем на первой части алгоритм обучается, а на второй проверяется. Размер отложенной выборки в данном случае нужно подбирать с осторожностью, так как слишком маленькая тестовая выборка не будет обеспечивать должной точности оценки качества обучения, а слишком маленькия обучающая выборка приведет к снижению качества обучения, так как будет малорепрезентативна. Таким образом, главный минус этого метода - сильная зависимость результата от того, как мы выбираем отложенную выборку. Например, в пространстве объектов могут быть какие-то особые, отличающиеся от остальных по какому-то свойству объекты, и может так произойти, что после разбиения они не попадут в обучающую выборку, алгоритм на них не обучится, и качество обучения после проверки на этих объектах, попавших в тестовую выборку, будет плохим. Один из путей решения этой проблемы - многократное случайное разбиение выборки на тестовую и обучающую и использование в качестве оценки качества среднего значения ошибки полученной после каждого разбиения. Но и этот метод не гарантирует, что каждый объект побывает в обучающей выборке, так как разбиения случайные.\n",
    "<img src=\"images/valid.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "2. **Кросс-валидация** (усложненная версия метода отложенной выборки). Этот метод как раз вытекает из проблемы, описанной выше. Он является более системным подходом. В этом случае выборка разбивается на $k$ блоков, и затем каждый из них по очереди используется в качестве тестового, а остальная часть - в качестве обучающей выборки. После прохождения всей выборки таким образом получается $k$ показателей качества, и итоговая оценка качества обучения по кросс-валидации оценивается как средняя из этих $k$. В этом случае мы гарантируем, что все данные поучаствуют в обучении. Выбор количества блоков $k$ обычно зависит от размера выборки. Чем больше данных, тем меньше нужно блоков, так как во-первых в этом случае после разбиения даже на малое количество блоков у нас остается большой объем данных в обучающей выборке, что обеспечивает хорошее качество обучения, а во-вторых, разбиение на $k$ блоков означает обучение алгоритма $k$ раз, соответственно, чем их больше, тем больше получается вычислительная сложность процесса обучения модели. Обычно $k$ принимает значение от 3 до 10.\n",
    "<img src=\"images/kfolds.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "3. Использовать **меры сложности модели**, позволяющие без дополнительной выборки выявить переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I6dPoI0iqum"
   },
   "source": [
    "Одним из знаков, что произошло переобучение модели, или _мерой сложности_ является получение больших по модулю весов при признаках. Посмотрим, что получилось в нашей последней модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4vCY2DJiqum",
    "outputId": "24e975e6-1b94-4518-d817-569837122aa3"
   },
   "outputs": [],
   "source": [
    "# выведем полученные веса при признаках и свободный коэффициент\n",
    "print(eighth_degree_regressor.named_steps.linearregression.coef_)\n",
    "print(eighth_degree_regressor.named_steps.linearregression.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CveLhp1iqup"
   },
   "source": [
    "Видим веса 2 и 3 порядков в то время как в кубичесой модели и в исходной зависимости ничего подобного не было. Это и говорит нам о том, что в данном случае имеет место переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_sCR6rXiqup"
   },
   "source": [
    "На этой особенности и основывается метод _регуляризации_ для борьбы с переобучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws1-N-_Qiqus"
   },
   "source": [
    "### Регуляризация <a class='anchor' id='reg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L_2$-регуляризация (ridge, регуляризация Тихонова)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Zgvr4aEiqut"
   },
   "source": [
    "Метод регуляризации заключается в \"штрафовании\" модели за слишком большие веса путем добавления нового члена к ошибке:\n",
    "\n",
    "$$Q(w, X) + \\lambda ||w||^{2} \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "добавленный член $\\lambda ||w||^{2}$ - **квадратичный регуляризатор**, который представляет собой $L_{2}$-норму вектора весов, то есть сумму квадратов весов $\\sum^{d}_{j=1}w_{j}^{2}$, коэффициент $\\lambda$ при нем - коэффициент регуляризации. Чем больше его значение, тем меньшая сложность модели будет получаться в процессе такого обучения. Если _увеличивать_ его, в какой-то момент оптимальным для модели окажется зануление всех весов. В то же время при слишком _низких_ его значениях появляется вероятность чрезмерного усложнения модели и переобучения. Выбор оптимального значения этого коэфициента является отдельной задачей и заключается в многократном обучении модели с разными его значениями и сравнении их качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP69lwhriqut"
   },
   "source": [
    "По сути, смысл регуляризации заключается, как и в обычном обучении, в минимизации функционала ошибки, только в данном случае добавляется условие непревышения нормой вектора весов некоторого значения $||w||^{2}\\leq C$, то есть ограничение весов, что и будет залогом избежания переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{cases} Q(w, X) \\rightarrow min \\\\ ||w||^2 \\leq C \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1,  1],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2],\n",
    "              [ 1,  5],\n",
    "              [ 1,  3],\n",
    "              [ 1,  0],\n",
    "              [ 1,  5],\n",
    "              [ 1, 10],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2]])\n",
    "\n",
    "y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графике ниже изображено изменение весов признаков при увеличении коэффициента регуляризации `alpha` от $10^{-3}$ до $100$ для модели `Ridge` (L2-регуляризация). Чем больше значение `alpha`, тем сильнее регуляризация и тем сильнее модель \"штрафует\" за большие значения весов, они убывают плавно и примерно одновременно друг с другом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "n = 50\n",
    "\n",
    "train_amount = 5\n",
    "train_X = X[:train_amount]\n",
    "train_y = y[:train_amount]\n",
    "test_X = X[train_amount:]\n",
    "test_y = y[train_amount:]\n",
    "\n",
    "\n",
    "coeffs = np.zeros((n, train_X.shape[1]))\n",
    "alpha_list = np.logspace(-3, 2, n)\n",
    "\n",
    "for i, val in enumerate(alpha_list):\n",
    "    ridge = Ridge(alpha=val, fit_intercept=False)\n",
    "    ridge.fit(train_X, train_y)\n",
    "    \n",
    "    coeffs[i, :] = ridge.coef_.flatten()\n",
    "\n",
    "for i in range(train_X.shape[1]):\n",
    "    plt.plot(alpha_list, coeffs[:, i])\n",
    "\n",
    "plt.title('Убывание абсолютных значений весов признаков\\n при увеличении коэффициента регуляризации alpha (Ridge)')\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Вес признака');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "\n",
    "coef = coeffs[index]\n",
    "\n",
    "print(f'Коэффициент регуляризации: {alpha_list[index]}')\n",
    "print(f'Веса: {coef}')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax1, ax2 = axs\n",
    "\n",
    "ax1.scatter(train_X[:, 1], train_y)\n",
    "\n",
    "y_pred = np.dot(train_X, coef)\n",
    "ax1.plot(train_X[:, 1], y_pred, c='r')\n",
    "\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('Y')\n",
    "print(f'MSE на обучении: {np.mean((y_pred - train_y)**2)}')\n",
    "\n",
    "ax2.scatter(test_X[:, 1], test_y)\n",
    "\n",
    "y_pred = np.dot(test_X, coef)\n",
    "ax2.plot(test_X[:, 1], y_pred, c='r')\n",
    "\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('Y')\n",
    "print(f'MSE на тесте: {np.mean((y_pred - test_y)**2)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L_1$-регуляризация (lasso, регуляризация через манхэттенское расстояние)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OTMLxupiquu"
   },
   "source": [
    "Описанный выше метод с использованием $L_{2}$-нормы вектора весов в качестве регуляризатора называется **$L_{2}$-регуляризацией**. По аналогии существует также **$L_{1}$-регуляризация**, использующая в качестве регуляризатора $L_{1}$-норму вектора весов, то есть сумму модулей весов.\n",
    "\n",
    "$$||w||_{1} = \\sum^{d}_{j=1}|w_{j}|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графике ниже изображено изменение весов признаков при увеличении коэффициента регуляризации `alpha` от $10^{-3}$ до $100$ для модели `Lasso` (L1-регуляризация). Чем больше значение `alpha`, тем сильнее регуляризация и тем сильнее модель \"штрафует\" за большую абсолютную величину признаков. \n",
    "\n",
    "Такой метод часто используется для отбора признаков: у менее ценных признаков гораздо раньше обнуляются веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "n = 50\n",
    "\n",
    "coeffs = np.zeros((n, train_X.shape[1]))\n",
    "alpha_list = np.logspace(-3, 2, n)\n",
    "\n",
    "for i, val in enumerate(alpha_list):\n",
    "    lasso = Lasso(alpha=val, fit_intercept=False)\n",
    "    lasso.fit(train_X, train_y)\n",
    "    coeffs[i, :] = lasso.coef_.flatten()\n",
    "\n",
    "for i in range(train_X.shape[1]):\n",
    "    plt.plot(alpha_list, coeffs[:, i])\n",
    "\n",
    "    \n",
    "plt.title('Убывание абсолютных значений весов признаков\\n при увеличении коэффициента регуляризации alpha (Lasso)')\n",
    "plt.xticks(np.arange(0, 101, 10))\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Вес признака');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 30\n",
    "\n",
    "coef = coeffs[index]\n",
    "\n",
    "print(f'Коэффициент регуляризации: {alpha_list[index]}')\n",
    "print(f'Веса: {coef}')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax1, ax2 = axs\n",
    "\n",
    "ax1.scatter(train_X[:, 1], train_y)\n",
    "\n",
    "y_pred = np.dot(train_X, coef)\n",
    "ax1.plot(train_X[:, 1], y_pred, c='r')\n",
    "\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('Y')\n",
    "print(f'MSE на обучении: {np.mean((y_pred - train_y)**2)}')\n",
    "\n",
    "ax2.scatter(test_X[:, 1], test_y)\n",
    "\n",
    "y_pred = np.dot(test_X, coef)\n",
    "ax2.plot(test_X[:, 1], y_pred, c='r')\n",
    "\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('Y')\n",
    "print(f'MSE на тесте: {np.mean((y_pred - test_y)**2)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJFR2HDliqu1"
   },
   "source": [
    "$L_{2}$-регуляризатор:\n",
    "* непрерывная гладкая функция\n",
    "* штрафует модель за сложность\n",
    "\n",
    "$L_{1}$-регуляризатор:\n",
    "* негладкая функция\n",
    "* занулении некоторых весов (отбор признаков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация создает некое абстрактное поле (зеленое) в градиентном спуске (овалы), из которого не может выйти обучение модели. Если мы увеличиваем Лямбду, то поле сужается.\n",
    "\n",
    "<img src='images/compare_l1_l2_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно применить одновременно L1 и L2 регуляризацию - это называется Elastic Net. Про это можно посмотреть [здесь](https://youtu.be/1dKRdX9bfIo) и почитать в оригинальной [статье](https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq5zA2Itiqu3"
   },
   "source": [
    "### Коэффициент детерминации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e7sgEO5iqu3"
   },
   "source": [
    "_Коэффициент детерминации_ $R^{2}$ является еще одной метрикой качества в задачах регрессии. Ранее мы говорили о средней абсолютной и среднеквадратичной ошибке. Коэффициент детерминации позволяет развить тему среднеквадратичной ошибки, интерпретируя ее. \n",
    "\n",
    "MSE не позволяет сама по себе сделать вывод о том, как хорошо модель решает задачу. Например, если целевая переманная принимает значения от 0 до 1, а MSE равняется 10, это плохой показатель, а когда целевая переменная варьируется от 1000 до 10000, такое же значение уже является очень хорошим. Для избавления от такой неясности и был введен коэффициент детерминации, который по сути является нормированной среднеквадратичной ошибкой и принимает значения от 0 до 1.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\frac{1}{n}\\sum^{l}_{i=1}{(y - y_{pred})^2}}{\\frac{1}{n}\\sum^{l}_{i=1}{(y - \\bar{y})^2}}$$\n",
    "\n",
    "где $\\bar{y}=\\frac{1}{l}\\sum^{l}_{i=1}y_{i}$ - среднее значение целевой переменной.\n",
    "\n",
    "Коэффициент детерминации характеризует, какую долю дисперсии ответов объясняет модель. Если $R^{2}=1$, то модель идеально описывает данные, если же $R^{2}$ близко к нулю, то предсказания сопоставимы по качеству с константной моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.array([0, 0.2, 0.1, 0.6, 0.3, 0.9, 0.7])\n",
    "pred = np.array([0.1, 0.3, 0.2, 0.7, 0.5, 0.5, 0.5])\n",
    "\n",
    "np.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.array([100, 20, 10, 60, 30, 90, 70])\n",
    "pred = np.array([110, 30, 20, 70, 50, 50, 50])\n",
    "\n",
    "np.mean((true - pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Практическая часть<a class=\"anchor\" id=\"practice\"></a><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_u5bV-OlT34p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача:__ предсказание баллов ЕГЭ ученика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_n6il-AZaLuA"
   },
   "outputs": [],
   "source": [
    "X = np.array([[   1,    1,  500,    1],\n",
    "              [   1,    1,  700,    1],\n",
    "              [   1,    2,  750,    2],\n",
    "              [   1,    5,  600,    1],\n",
    "              [   1,    3, 1450,    2],\n",
    "              [   1,    0,  800,    1],\n",
    "              [   1,    5, 1500,    3],\n",
    "              [   1,   10, 2000,    3],\n",
    "              [   1,    1,  450,    1],\n",
    "              [   1,    2, 1000,    2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8yoci01ni-t"
   },
   "outputs": [],
   "source": [
    "y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ei5qeZO_aT6m",
    "outputId": "087819d3-202a-46c9-f87d-49e17953099c"
   },
   "outputs": [],
   "source": [
    "X[:, 1].min(), X[:, 1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L7wsW1Q7b4Ts",
    "outputId": "911044d6-32c1-475a-80cd-cbaf26d5ba32"
   },
   "outputs": [],
   "source": [
    "X[:, 2].min(), X[:, 2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X):\n",
    "    return (X - X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "E55S9Bzob9Xa",
    "outputId": "4abf56a2-d484-4c24-8520-8ed1b58ccca8"
   },
   "outputs": [],
   "source": [
    "X_norm = X.copy()\n",
    "X_norm = X_norm.astype(np.float64)\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "7x-6IuPOcIo0",
    "outputId": "1668fec7-37ae-4671-ad9f-f093c30a6c72"
   },
   "outputs": [],
   "source": [
    "X_norm[:, 1] = min_max_scale(X_norm[:, 1])\n",
    "X_norm[:, 2] = min_max_scale(X_norm[:, 2])\n",
    "X_norm[:, 3] = min_max_scale(X_norm[:, 3])\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zy63bpFfeqzu"
   },
   "source": [
    "Стандартизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "8i9pB-3-fMD4",
    "outputId": "3cbe76f6-d000-4fcd-fe86-ffe853403356"
   },
   "outputs": [],
   "source": [
    "print(np.mean(X[:, 1]))\n",
    "plt.hist(X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "-bnAYQqVfRX-",
    "outputId": "d59b0013-49bc-40fa-cb29-e71403e6e9e0"
   },
   "outputs": [],
   "source": [
    "print(np.mean(X[:, 2]))\n",
    "plt.hist(X[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(X):\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    return (X - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M57-Pzl_f_rF"
   },
   "outputs": [],
   "source": [
    "X_st = X.copy().astype(np.float64)\n",
    "X_st[:, 1] = standard_scale(X_st[:, 1])\n",
    "X_st[:, 2] = standard_scale(X_st[:, 2])\n",
    "X_st[:, 3] = standard_scale(X_st[:, 3])\n",
    "\n",
    "X_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "-Y1tzWGCgYjA",
    "outputId": "89dc21fe-180f-4e68-f30f-1cf276c4f154"
   },
   "outputs": [],
   "source": [
    "print(np.mean(X_st[:, 1]))\n",
    "print(np.std(X_st[:, 1]))\n",
    "\n",
    "plt.hist(X_st[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_st[:, 2]))\n",
    "print(np.std(X_st[:, 2]))\n",
    "\n",
    "plt.hist(X_st[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdGy9tjVhZol"
   },
   "source": [
    "SGD (Stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDpN7UxYnrcS"
   },
   "outputs": [],
   "source": [
    "def calc_mse(y, y_pred):\n",
    "    err = np.mean((y - y_pred)**2)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(X.shape[1])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgEyis2zlJeb"
   },
   "outputs": [],
   "source": [
    "# классический  GD\n",
    "def gradient_descent(X, y, iterations, eta=1e-4):\n",
    "    W = np.random.randn(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        y_pred = np.dot(X, W)\n",
    "        err = calc_mse(y, y_pred)\n",
    "        dQ = 2/n * X.T @ (y_pred - y) # градиент функции ошибки\n",
    "        W -= (eta * dQ)\n",
    "        if i % (iterations / 10) == 0:\n",
    "            print(f'Iter: {i}, weights: {W}, error {err}')\n",
    "    print(f'Final MSE: {calc_mse(y, np.dot(X, W))}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "4nsYMAGjmFlB",
    "outputId": "8959a8af-3d27-42a6-afae-5db46c794fe1"
   },
   "outputs": [],
   "source": [
    "gradient_descent(X_st, y, iterations=5000, eta=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стохастический градиентный спуск\n",
    "def stohastic_gradient_descent(X, y, iterations, batch_size, eta=1e-4):\n",
    "    W = np.random.randn(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    n_batch = n // batch_size    \n",
    "    if n % batch_size != 0:\n",
    "        n_batch += 1\n",
    "    print(f'amount of batches is {n_batch}')\n",
    "        \n",
    "    for i in range(0, iterations):\n",
    "        \n",
    "        for b in range(n_batch):\n",
    "            start = batch_size * b\n",
    "            end = batch_size * (b + 1)\n",
    "            \n",
    "#             print(b, start, end)\n",
    "            \n",
    "            X_tmp = X[start : end, ]\n",
    "            y_tmp = y[start : end]\n",
    "\n",
    "            err = calc_mse(y, np.dot(X, W))\n",
    "            \n",
    "            y_pred_tmp = np.dot(X_tmp, W)\n",
    "            dQ = 2/len(y_tmp) * X_tmp.T @ (y_pred_tmp - y_tmp) # градиент функции ошибки\n",
    "            W -= (eta * dQ)\n",
    "        \n",
    "        if i % (iterations / 10) == 0:\n",
    "            print(f'Iter: {i}, weights: {W}, error {err}')\n",
    "    \n",
    "    print(f'Final MSE: {calc_mse(y, np.dot(X, W))}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "2aXap4Vsm3GV",
    "outputId": "8b666b76-9d01-4b6d-813c-f1d3f5901f5f"
   },
   "outputs": [],
   "source": [
    "stohastic_gradient_descent(X_st, y, iterations=5000, batch_size=4, eta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYTxT_6OrZpe"
   },
   "source": [
    "##### L1 регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ojkn7xKrsyiw"
   },
   "source": [
    "$$Q(w, X) + \\lambda |w| \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "Про производную можно посмотреть [здесь](https://youtu.be/dHhYHGI9E6I)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j38vId7Hvtg0"
   },
   "source": [
    "##### L2 регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(w, X) + \\lambda ||w||^{2} \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "<center>Производная</center>\n",
    "\n",
    "$$d\\lambda ||w||^{2} = 2\\lambda w $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AVoe1Mswjni"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_reg_l2(X, y, iterations, eta=1e-4, reg=1e-8):\n",
    "    W = np.random.randn(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        y_pred = np.dot(X, W)\n",
    "        err = calc_mse(y, y_pred)\n",
    "        \n",
    "        dQ = 2/n * X.T @ (y_pred - y) # градиент функции ошибки\n",
    "        dReg = reg * W # градиент регуляризации\n",
    "        \n",
    "        W -= eta * (dQ + dReg)\n",
    "        \n",
    "        if i % (iterations / 10) == 0:\n",
    "            print(f'Iter: {i}, weights: {W}, error {err}')\n",
    "    \n",
    "    print(f'Final MSE: {calc_mse(y, np.dot(X, W))}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "vLJL6j8Lw6OA",
    "outputId": "dac9ef42-5966-4404-9346-aeb18cca83c6"
   },
   "outputs": [],
   "source": [
    "gradient_descent_reg_l2(X_st, y, iterations=5000, eta=1e-2, reg=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "4nsYMAGjmFlB",
    "outputId": "8959a8af-3d27-42a6-afae-5db46c794fe1"
   },
   "outputs": [],
   "source": [
    "gradient_descent(X_st, y, iterations=5000, eta=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание <a class='anchor' id='hw'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Постройте график зависимости весов всех признаков от lambda в самописной L2-регуляризации (на данных про ЕГЭ). Сделайте вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "X = np.array([[   1,    1,  500,    1],\n",
    "              [   1,    1,  700,    1],\n",
    "              [   1,    2,  750,    2],\n",
    "              [   1,    5,  600,    1],\n",
    "              [   1,    3, 1450,    2],\n",
    "              [   1,    0,  800,    1],\n",
    "              [   1,    5, 1500,    3],\n",
    "              [   1,   10, 2000,    3],\n",
    "              [   1,    1,  450,    1],\n",
    "              [   1,    2, 1000,    2]])\n",
    "\n",
    "n = 50\n",
    "\n",
    "train_amount = 5\n",
    "train_X = X[:train_amount]\n",
    "train_y = y[:train_amount]\n",
    "test_X = X[train_amount:]\n",
    "test_y = y[train_amount:]\n",
    "\n",
    "\n",
    "coeffs = np.zeros((n, train_X.shape[1]))\n",
    "alpha_list = np.logspace(-3, 2, n)\n",
    "\n",
    "for i, val in enumerate(alpha_list):\n",
    "    ridge = Ridge(alpha=val, fit_intercept=False)\n",
    "    ridge.fit(train_X, train_y)\n",
    "    \n",
    "    coeffs[i, :] = ridge.coef_.flatten()\n",
    "\n",
    "for i in range(train_X.shape[1]):\n",
    "    plt.plot(alpha_list, coeffs[:, i])\n",
    "\n",
    "plt.title('Убывание абсолютных значений весов признаков\\n при увеличении коэффициента регуляризации alpha (Ridge)')\n",
    "plt.xticks(np.arange(0, 101, 5))\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Вес признака');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "чем больше коэфф регуляризации, тем бестрее веса признаков стремяться к минимуму(нулю) , чем меньше коэфф, тем бысрее веса стремяться к бесконечности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишите функцию наподобие gradient_descent_reg_l2, но для применения L1-регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_reg_l1(X, y, iterations, eta=1e-4,reg=1):\n",
    "    W = np.random.random(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        y_pred = np.dot(X, W)\n",
    "        err = calc_mse(y, y_pred)\n",
    "        \n",
    "        dQ = 2/n * X.T @ (y_pred - y) # градиент функции ошибки\n",
    "        # dReg = reg * W # градиент регуляризации\n",
    "        # Изменено\n",
    "        dReg=reg * (W / abs(W))\n",
    "        W -= eta * (dQ + dReg)\n",
    "        # Изменено\n",
    "        if i % (iterations / 10) == 0:\n",
    "            print(f'Iter: {i}, weights: {W}, error {err}')\n",
    "    \n",
    "    print(f'Final MSE: {calc_mse(y, np.dot(X, W))}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, weights: [0.88467602 0.36355424 0.86834162 0.36013888], error 3235.2438166517086\n",
      "Iter: 500, weights: [36.02911806  4.01721812  3.70098371  3.76468698], error 449.66941469716966\n",
      "Iter: 1000, weights: [48.94509605  4.6340301   3.47022466  4.1335794 ], error 84.83693985606455\n",
      "Iter: 1500, weights: [53.69186333  4.94961993  3.07022368  4.29506599], error 34.867758630385374\n",
      "Iter: 2000, weights: [55.43635373  5.184247    2.70515384  4.44880103], error 27.659368517310345\n",
      "Iter: 2500, weights: [56.07747359  5.36819205  2.38283035  4.6034851 ], error 26.377207408794344\n",
      "Iter: 3000, weights: [56.31309235  5.51423524  2.09797939  4.75568986], error 25.979996594435228\n",
      "Iter: 3500, weights: [56.3996849   5.63125062  1.84543123  4.90249465], error 25.755532024670686\n",
      "Iter: 4000, weights: [56.43150863  5.7258702   1.62085987  5.04202319], error 25.591239015718667\n",
      "Iter: 4500, weights: [56.44320421  5.80310008  1.42066093  5.1731934 ], error 25.462270973351078\n",
      "Final MSE: 25.358969042074257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([56.44749747,  5.8666178 ,  1.24214588,  5.29525116])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_reg_l1(X_st, y, iterations=5000, eta=1e-3,reg=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. *Можно ли к одному и тому же признаку применить сразу и нормализацию, и стандартизацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##останется последний примененный вариант,данные либо меняются либо от 0 до 1,либо имеют среднее в нуле+ст.откл=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osNM_LLkiqu6"
   },
   "source": [
    "4. *Сгенерируйте датасет при помощи <code>sklearn.datasets.make_regression</code> и обучите линейную модель при помощи градиентного и стохастического градиентного спуска. Нанесите среднеквадратичную ошибку для обоих методов на один график, сделайте выводы о разнице скорости сходимости каждого из методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проект: \n",
    "1. https://www.kaggle.com/c/gb-tutors-expected-math-exam-results регрессия\n",
    "1. https://www.kaggle.com/c/gb-classification-choose-tutors классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esNMM7hXiqu4"
   },
   "source": [
    "## Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPQPnl--iqu4"
   },
   "source": [
    "1. [Стохастический градиентный спуск](http://www.machinelearning.ru/wiki/index.php?title=%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "2. [sklearn.datasets.make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)\n",
    "3. [numpy.mean](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.mean.html)\n",
    "4. [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "5. [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "6. [sklearn.pipeline.make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Масштабирование_\n",
    "* Масштабирование признаков - хорошая практика, позволяющая обучать модели быстрее и делающая их более точными\n",
    "* При использовании метрических алгоритмов масштабирование обязательно (!)\n",
    "\n",
    "_Стохастический градиентный спуск_\n",
    "* Стохастический градиентный спуск (SGD) - на каждом шаге уменьшаем ошибку только на одном объекте (или нескольких), а не на всей выборке, работает быстрее обычного GD, т.к. меньше вычислений\n",
    "* SGD из-за стохастичности может \"перепрыгнуть\" локальный минимум и попасть в глобальный\n",
    "\n",
    "_Регуляризация_\n",
    "* Переобучение - модель \"выучила\" выборку, но обобщающая способность слабая\n",
    "* Признаки переобучения: качество на трейне высокое, а на тесте низкое, большие веса модели\n",
    "* Один из способов борьбы с переобучением - регуляризация - штраф за большие веса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определения\n",
    "*Масштабирование данных*\n",
    "\n",
    "**Нормализация данных** — метод предобработки числовых признаков в обучающих наборах данных с целью приведения их к некоторой общей шкале без потери информации о различии диапазонов. (шкала от 0 до 1)\n",
    "\n",
    "**Стандартизация** — метод предобработки с целью приведения данных к единому формату и представлению. (М=0, std=1)\n",
    "___________\n",
    "_Стохастический градиентный спуск_\n",
    "\n",
    "**Стохастический градиентный спуск** — метод нахождения локального экстремума функции (минимума или максимума) с помощью движения вдоль градиента, который считается на каждом шаге не как сумма градиентов от каждого элемента выборки, а как градиент от одного, случайно выбранного элемента.\n",
    "\n",
    "___________\n",
    "_Переобучение_\n",
    "\n",
    "**Переобучение** (overfitting) — явление, когда алгоритм хорошо объясняет примеры из обучающей выборки (обеспечивает малую величину ошибки), но плохо работает на примерах, не участвовавших в обучении (не обеспечивает малую величину ошибки).\n",
    "\n",
    "**Недообучение** (underfitting) — явление, при котором алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке.\n",
    "\n",
    "**Кросс-валидация** — процедура оценивания обобщающей способности алгоритмов. С её помощью эмулируется наличие тестовой выборки, которая не участвует в обучении, но для которой известны правильные ответы.\n",
    "\n",
    "________\n",
    "_Регуляризация_\n",
    "\n",
    "**Регуляризация** — метод добавления некоторых дополнительных ограничений к условию с целью решить некорректно поставленную задачу или предотвратить переобучение. Эта информация часто имеет вид штрафа за сложность модели.\n",
    "$$Q(w, X) + \\lambda ||w||^{2} \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "**L1-регуляризация** (lasso, регуляризация через манхэттенское расстояние) — метод добавления дополнительных ограничений в виде $L_{1}$-норму вектора весов, то есть сумму модулей весов. \n",
    "$$||w||_{1} = \\sum^{d}_{j=1}|w_{j}|.$$\n",
    "\n",
    "**L2-регуляризация** (ridge, регуляризация Тихонова) — метод добавления дополнительных ограничений в виде $L_{2}$-норму вектора весов, то есть сумму квадратов весов. \n",
    "$$||w||_{2} = \\sum^{d}_{j=1}w^{2}.$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ws1-N-_Qiqus",
    "Lq5zA2Itiqu3"
   ],
   "name": "Lesson_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
